{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8718078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import cv2\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def load_digitalin(path,nchannels=16,Fs = 20000):\n",
    "    import pandas as pd\n",
    "    \n",
    "    digital_word = np.fromfile(path,'uint16')\n",
    "    sample = len(digital_word)\n",
    "    time = np.arange(0,sample)\n",
    "    time = time/Fs\n",
    "\n",
    "    \n",
    "    for i in range(nchannels):\n",
    "        if i == 0: data = (digital_word & 2**i)>0\n",
    "        else: data = np.vstack((data,(digital_word & 2**i)>0))\n",
    "\n",
    "    return data\n",
    "\n",
    "def CountTTL(TTL):\n",
    "    #Return the number TTL and the index where the last one starts\n",
    "    \n",
    "    TTL = list(map(int,TTL))\n",
    "    diff_TTL = np.diff(TTL)\n",
    "    \n",
    "    t_start = np.where(diff_TTL == 1)\n",
    "\n",
    "    return(len(t_start[0]),t_start[0][-1])\n",
    "\n",
    "def CountFrames(path):\n",
    "    #Return the number of frames inside a video\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    return(int(cap.get(cv2.CAP_PROP_FRAME_COUNT)))\n",
    "\n",
    "def TTLtoTimes(TTL,Fs = 20000):\n",
    "    \n",
    "    if isinstance(TTL[0],(np.bool_,bool)):\n",
    "        TTL = list(map(int,TTL))\n",
    "    \n",
    "    diff_TTL = np.diff(TTL)\n",
    "\n",
    "    t_start = np.where(diff_TTL == 1)[0]\n",
    "    t_end = np.where(diff_TTL == -1)[0]\n",
    "    t_TTL = np.array([np.mean(interval) for interval in zip(t_start,t_end)])\n",
    "    \n",
    "    return t_TTL/Fs\n",
    "\n",
    "def xml(session):\n",
    "    tree = ET.parse(session+'.xml')\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    xmlInfo = {}\n",
    "    for elem in root:\n",
    "        for subelem in elem:\n",
    "            try: \n",
    "                xmlInfo.update({subelem.tag:int(subelem.text)})\n",
    "            except:\n",
    "                pass\n",
    "    return xmlInfo\n",
    "\n",
    "def concat_lfp_dat(session,path_dat,subsessions):\n",
    "    \n",
    "    print('Starting dat concatenation')\n",
    "    \n",
    "    to_cat = \" + \".join(path_dat)\n",
    "    p = subprocess.Popen('copy /B ' + to_cat + \" \" + session+'.dat&',shell = True)\n",
    "    \n",
    "    #Check if concatenated file is not corrupted : \n",
    "    originalFileSize = []\n",
    "    for p in path_dat:\n",
    "        originalFileSize.append(os.path.getsize(p))\n",
    "    finalSize = np.sum(originalFileSize)\n",
    "    f = False\n",
    "    while not f:\n",
    "        f = os.path.exists(session+'.dat')\n",
    "        \n",
    "    pbar = tqdm(total = finalSize,desc = 'Concat : ')\n",
    "    concatSize = 0\n",
    "    while concatSize < finalSize:\n",
    "        time.sleep(10)\n",
    "        s = os.path.getsize(session+'.dat')\n",
    "        \n",
    "        if concatSize == s:\n",
    "            print('Concatenation in stuck try again')\n",
    "            p.kill()\n",
    "            break\n",
    "        \n",
    "        pbar.update(s-concatSize)\n",
    "        concatSize = s\n",
    " \n",
    "    originalFileSize = []\n",
    "    for p in path_dat:\n",
    "        originalFileSize.append(os.path.getsize(p))\n",
    "    finalSize = np.sum(originalFileSize)\n",
    "    concatSize = os.path.getsize(session+'.dat')\n",
    "    \n",
    "    \n",
    "    if finalSize == concatSize: \n",
    "        print('Concat Succefull')\n",
    "        print('Creating cat event file')\n",
    "        \n",
    "        t_rec = [0]\n",
    "        for p in path_dat:\n",
    "            t = os.path.getsize(p)/(fs*nChannels*bytePerSample)\n",
    "            print(t)\n",
    "            t_rec.append(t)\n",
    "        concat_event = np.cumsum(t_rec)*1000 # 1000 is for conversion in ms\n",
    "        \n",
    "        with open(session+'.cat.evt','w') as f:\n",
    "            for i,c in enumerate(concat_event,1):\n",
    "                f.write(str(concat_event[i-1])+ f'{subsessions[i]} Start \\n')\n",
    "                f.write(str(concat_event[i])+ f'{subsessions[i]} End \\n')\n",
    "        return 1\n",
    "    else:\n",
    "        print('Error, please try again. Check freespace on your drives. You need at least',np.sum(originalFileSize)/1e9,'GB')\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "def concat_digitalin(session,path_digitalin,path_videos,nchannels = 16):\n",
    "    \n",
    "    byteSize = 2\n",
    "    print('Starting digitalin concatenation')\n",
    "    \n",
    "    originalFileSize = []\n",
    "    for p in path_digitalin:\n",
    "        originalFileSize.append(os.path.getsize(p))\n",
    "    finalSize = np.sum(originalFileSize)\n",
    "    \n",
    "    \n",
    "    data = np.empty((16,0),dtype = np.bool)\n",
    "    \n",
    "    pbar = tqdm(total = int(finalSize/byteSize))\n",
    "    # Reading all digitalin files in sub-session and comparate number of TTL with number of frames : \n",
    "    \n",
    "    #Logs\n",
    "    with open(f'{session}-preprocess.log','a') as log_file:\n",
    "            log_file.write(f'\\n\\nNUMBER OF TTL AND TTLs CORRECTIONS\\n\\n')\n",
    "            log_file.close()\n",
    "            \n",
    "    for p_digitalin,p_video in zip(path_digitalin,path_videos):\n",
    "        data_ = load_digitalin(p_digitalin,nchannels)\n",
    "        nTTL,lTTL = CountTTL(data_[0,:])\n",
    "        nFrames = CountFrames(p_video)\n",
    "        time.sleep(0.1)\n",
    "        if nTTL == nFrames: # Same number of frames and TTL is easy case, we don't do anaything\n",
    "            print('NTTL',nTTL)\n",
    "            print('Number of Frames',nFrames)\n",
    "            print('Do not need to correct last TTL')\n",
    "        elif nTTL-1 == nFrames: # If NTTL and Frames are only one appart we need to delete the last TTL\n",
    "            print('NTTL',nTTL)\n",
    "            print('Number of Frames',nFrames)\n",
    "            print('Last TTL was at ',lTTL)\n",
    "            print('Correcting TTLs :')\n",
    "            data_[0,lTTL:] = False\n",
    "            print('Counting new number of TTL : ' , CountTTL(data_[0,:])[0])\n",
    "        else:\n",
    "            print('NTTL',nTTL)\n",
    "            print('Number of Frames',nFrames)\n",
    "            print('Last TTL was at ',lTTL)\n",
    "            print('Some file might be corrupted, need to implement this')\n",
    "        oldnTTL = nTTL\n",
    "        nTTL = CountTTL(data_[0,:])[0]\n",
    "        \n",
    "       #Logs \n",
    "        with open(f'{session}-preprocess.log','a') as log_file:\n",
    "            log_file.write(f'{p_digitalin} - {p_video}\\n')\n",
    "            log_file.write(f'NTTL in digitalin file :                        {oldnTTL}\\n')\n",
    "            log_file.write(f'nFrames in video file :                         {nFrames}\\n')\n",
    "            log_file.write(f'Number of TTL after TTL corrections :           {nTTL}\\n')\n",
    "            log_file.write(f'Delta Frames in the end                         {nTTL-nFrames}\\n\\n')\n",
    "            log_file.close()\n",
    "        \n",
    "        # Anyway we concatenate digitalin in the RAM\n",
    "        print('Concatenating digitalin in RAM')\n",
    "        data = np.hstack((data,data_))\n",
    "        time.sleep(0.1)\n",
    "        pbar.update(np.size(data_))\n",
    "        time.sleep(0.1)\n",
    "\n",
    "\n",
    "    del data_ #delete temp var for better managing of RAM\n",
    "\n",
    "    #Recreate the write format of digital in (16channels inside a 16bits uint16)\n",
    "    digitalwrite = np.empty(data.shape[1],dtype = np.uint16)\n",
    "    for i in range(nchannels):\n",
    "        digitalwrite += np.uint16((data[i,:] * 2**i))\n",
    "\n",
    "    print('Writting digitalin.dat')\n",
    "    f = open('digitalin.dat','wb')\n",
    "    f.write(digitalwrite)\n",
    "    f.close()\n",
    "    \n",
    "    \n",
    "    #Check if concatenated file is not corrupted : \n",
    "\n",
    "    concatSize = os.path.getsize('digitalin.dat')\n",
    "    \n",
    "    if finalSize == concatSize: \n",
    "        print('Concat Succefull')\n",
    "        \n",
    "        \n",
    "        timestamps = TTLtoTimes(data[0,:])\n",
    "        np.save(session+'-frametime',timestamps)\n",
    "        del timestamps\n",
    "        del data\n",
    "        return 1   \n",
    "    else:\n",
    "        print('Error, please try again')\n",
    "        return 0\n",
    "    \n",
    "\n",
    "def downsampleDatFileBK(path, n_channels, fs):\n",
    "    \n",
    "    #Code by Guillaume Viejo \n",
    "    #Adapted by Billel Khouader oct 2020\n",
    "    \n",
    "    \"\"\"\n",
    "    downsample .dat file to .lfp 1/16 (20000 -> 1250 Hz)\n",
    "    \n",
    "    Since .dat file can be very big, the strategy is to load one channel at the time,\n",
    "    downsample it, and free the memory.\n",
    "    \n",
    "    BK : Modified in order to load dat chunk by chunk in write it as LFP file without \n",
    "    loading nChannels time the original dat file\n",
    "    \n",
    "    Args:\n",
    "        path: string\n",
    "        n_channel: int\n",
    "        fs: int\n",
    "    Return: \n",
    "        none\n",
    "    \"\"\"    \n",
    "    if not os.path.exists(path):\n",
    "        print(\"The path \"+path+\" doesn't exist; Exiting ...\")\n",
    "        sys.exit()\n",
    "    listdir     = os.listdir(path)\n",
    "    datfile     = [f for f in listdir if (f.startswith('Rat') & f.endswith('.dat'))]\n",
    "    if not len(datfile):\n",
    "        print(\"Folder contains no xml files; Exiting ...\")\n",
    "        sys.exit()\n",
    "    new_path = os.path.join(path, datfile[0])\n",
    "\n",
    "    f             = open(new_path, 'rb')\n",
    "    startoffile   = f.seek(0, 0)\n",
    "    endoffile     = f.seek(0, 2)\n",
    "    bytes_size    = 2\n",
    "    n_samples     = int((endoffile-startoffile)/n_channels/bytes_size)\n",
    "    duration      = n_samples/fs\n",
    "    f.close()\n",
    "\n",
    "    chunksize = 100_000\n",
    "    \n",
    "    lfp = np.zeros((int(n_samples/16),n_channels),dtype = np.int16)\n",
    "    \n",
    "    # Loading\n",
    "    \n",
    "    count = 0\n",
    "    f_lfp = open(session+'.lfp', 'wb')\n",
    "    \n",
    "    pbar = tqdm(total = n_samples,desc = 'Extracting LFP file from DAT file (sample)')\n",
    "    while count < n_samples:\n",
    "#         print(count)\n",
    "        f             = open(new_path, 'rb')\n",
    "        seekstart     = count*n_channels*bytes_size\n",
    "        f.seek(seekstart)\n",
    "        block         = np.fromfile(f, np.int16, n_channels*np.minimum(chunksize, n_samples-count))\n",
    "        f.close()\n",
    "        block         = block.reshape(np.minimum(chunksize, n_samples-count), n_channels)\n",
    "        count         += chunksize\n",
    "        \n",
    "    # Downsampling        \n",
    "#         lfp     = scipy.signal.resample_poly(block, 1, 16)\n",
    "        lfp     = scipy.signal.decimate(block,16,axis = 0)\n",
    "#         lfp     = scipy.signal.decimate(lfp,4,axis = 0)\n",
    "#         lfp     = scipy.signal.resample(block, int(len(block)/16))\n",
    "\n",
    "        f_lfp.write(lfp.astype('int16'))\n",
    "        pbar.update(chunksize)\n",
    "\n",
    "        \n",
    "        del block\n",
    "        del lfp\n",
    "    f_lfp.close()  \n",
    "    return\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a778887e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Rat25-210407-preprocess.log file\n",
      "Rat25-210407.md\n",
      "Rat25-210407.md was found\n",
      "2  subsessions were found\n",
      "     pre_sleep_1\n",
      "     post_sleep_1\n",
      "2  dat files were found\n",
      "     Y:\\Rat25\\Rat25-210407\\Rat25_210407_101231\\amplifier_analogin_auxiliary_int16.dat\n",
      "     Y:\\Rat25\\Rat25-210407\\Rat25_210407_135509\\amplifier_analogin_auxiliary_int16.dat\n",
      "Files be concatenated in this order.\n",
      "2  digitalin files were found\n",
      "     Y:\\Rat25\\Rat25-210407\\Rat25_210407_101231\\digitalin.dat\n",
      "     Y:\\Rat25\\Rat25-210407\\Rat25_210407_135509\\digitalin.dat\n",
      "2  Videos were founds : \n",
      "     Y:\\Rat25\\Rat25-210407\\Rat25_210407_101231\\Basler_acA1300-200uc__23039139__20210407_101236101.mp4\n",
      "     Y:\\Rat25\\Rat25-210407\\Rat25_210407_135509\\Basler_acA1300-200uc__23039139__20210407_135514279.mp4\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Rat25-210407.dat already exist do you want to overwrite ? (Y/N) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenation of  Rat25-210407.dat aborted\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "digitalin.dat already exist do you want to overwrite ? (Y/N) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenation of digitalin aborted\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Rat25-210407.lfp already exist do you want to overwrite ? (Y/N) n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenation of digitalin aborted\n",
      "Dat concatenation done in 0.0 s\n",
      "Digital in concatenation done in 0.0 s\n",
      "LFP extraction done in 0.0 s\n",
      "Session's length base on LFP dat file 27296.568 s\n",
      "Session's length base on digitalin.dat file 27296.568 s\n",
      "Session's length base on downsampled file 27296.568 s\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(path):\n",
    "    print(path, ' does not exists please check directory','Exiting...')\n",
    "    sys.exit()\n",
    "\n",
    "session = path.rsplit('\\\\')[-1]\n",
    "os.chdir(path)\n",
    "\n",
    "print(f'Creating {session}-preprocess.log file')\n",
    "#Logs\n",
    "with open(f'{session}-preprocess.log','a') as log_file:\n",
    "        now = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "        log_file.write(f'{now} : Starting preprocessing of {session}\\n')\n",
    "        log_file.close()\n",
    "\n",
    "path_md = f'{session}.md'\n",
    "print(path_md)\n",
    "if os.path.exists(path_md):\n",
    "    with open(path_md,'r') as f:\n",
    "        md_file = f.readlines()\n",
    "    subsessions = [re.findall('(?<=# )(.*)',l) for l in md_file]\n",
    "    subsessions = [s[0] for s in subsessions if s]\n",
    "    print(f'{path_md} was found')\n",
    "    print(len(subsessions),' subsessions were found')\n",
    "    for s in subsessions: print(\"    \",s)\n",
    "\n",
    "\n",
    "    with open(f'{session}-preprocess.log','a') as log_file:\n",
    "        now = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "        log_file.write(f'\\n{now} : MD files {path_md}\\n')\n",
    "        for s in subsessions: log_file.write(f'{s}\\n')\n",
    "        log_file.close()\n",
    "else:\n",
    "    print('Could not find md file, are you taking notes ?')\n",
    "\n",
    "# Looking for Dat (LFPs) files \n",
    "path_dat = [[os.path.join(path,p,f) \n",
    "             for f in os.listdir(os.path.join(path,p)) if f.startswith(\"amplifier_analogin_auxiliary\") & f.endswith('.dat')][0] \n",
    "            for p in os.listdir(path) if os.path.isdir(p)]\n",
    "print(len(path_dat),' dat files were found')\n",
    "for p in path_dat: print(\"    \",p)\n",
    "\n",
    "#Logs\n",
    "with open(f'{session}-preprocess.log','a') as log_file:\n",
    "    now = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    log_file.write(f'\\n{now} : DAT FILES \\n')\n",
    "    for p in path_dat: log_file.write(f'{p}\\n')\n",
    "    log_file.close()\n",
    "\n",
    "\n",
    "\n",
    "print('Files be concatenated in this order.')\n",
    "\n",
    "\n",
    "# Looking for digitalin.dat (TTLs and all digital signal)\n",
    "path_digitalin = [[os.path.join(path,p,f) \n",
    "             for f in os.listdir(os.path.join(path,p)) if f.startswith(\"digitalin\") & f.endswith('.dat')][0] \n",
    "            for p in os.listdir(path) if os.path.isdir(p)]\n",
    "print(len(path_digitalin),' digitalin files were found')\n",
    "for p in path_digitalin: print(\"    \",p)\n",
    "\n",
    "with open(f'{session}-preprocess.log','a') as log_file:\n",
    "    now = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    log_file.write(f'\\n{now} : DIGITALIN FILES\\n')\n",
    "    for p in path_digitalin: log_file.write(f'{p}\\n')\n",
    "    log_file.close()\n",
    "\n",
    "\n",
    "#Looking for videos (usefull for counting frames)\n",
    "path_videos = [[os.path.join(path,p,f) \n",
    "             for f in os.listdir(os.path.join(path,p)) if f.startswith(\"Basler\") & f.endswith('.mp4')][0] \n",
    "            for p in os.listdir(path) if os.path.isdir(p)]\n",
    "print(len(path_videos),\" Videos were founds : \")\n",
    "for p in path_videos: print(\"    \",p)\n",
    "#Logs    \n",
    "with open(f'{session}-preprocess.log','a') as log_file:\n",
    "    now = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    log_file.write(f'\\n{now} : VIDEOS FILES\\n')\n",
    "    for p in path_videos: log_file.write(f'{p}\\n')\n",
    "    log_file.close()\n",
    "\n",
    "if not (len(path_dat)==len(path_digitalin)==len(path_videos)): \n",
    "    print('Not same number of dat, digitalin or videos, please check integrity of each subsession')\n",
    "#Logs\n",
    "    with open(f'{session}-preprocess.log','a') as log_file:\n",
    "        now = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "        log_file.write(f'\\n{now} : WARNING\\n')\n",
    "        for p in path_videos: log_file.write(f'Not same number of dat, digitalin or videos, please check integrity of each subsession')\n",
    "        log_file.close()\n",
    "    raise ValueError\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "xmlInfo = xml(session)\n",
    "\n",
    "fs = xmlInfo['samplingRate']\n",
    "nChannels = xmlInfo['nChannels']\n",
    "bytePerSample = xmlInfo['nBits']/8\n",
    "\n",
    "\n",
    "files = os.listdir()\n",
    "if session+'.dat' in files:\n",
    "    overwrite = input(session+'.dat' + ' already exist do you want to overwrite ? (Y/N)')\n",
    "    if overwrite.lower() == \"y\": writeDat = True\n",
    "    else: \n",
    "        writeDat = False\n",
    "        print('Concatenation of ',session+'.dat aborted')\n",
    "else: writeDat = True\n",
    "\n",
    "if 'digitalin.dat' in files:\n",
    "    overwrite = input('digitalin.dat' + ' already exist do you want to overwrite ? (Y/N)')\n",
    "    if overwrite.lower() == \"y\": writeDigital = True\n",
    "    else: \n",
    "        writeDigital = False\n",
    "        print('Concatenation of digitalin aborted')\n",
    "else: writeDigital = True\n",
    "\n",
    "\n",
    "if session+'.lfp' in files:\n",
    "    overwrite = input(session+'.lfp' + ' already exist do you want to overwrite ? (Y/N)')\n",
    "    if overwrite.lower() == \"y\": extractLFP = True\n",
    "    else: \n",
    "        extractLFP = False\n",
    "        print('Concatenation of digitalin aborted')\n",
    "else: extractLFP = True\n",
    "\n",
    "\n",
    "############\n",
    "#DAT CONCAT#\n",
    "############    \n",
    "t = time.time()\n",
    "if writeDat:\n",
    "    with open(f'{session}-preprocess.log','a') as log_file:\n",
    "        now = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "        log_file.write(f'\\n{now} : Starting Dat concatenation\\n')\n",
    "        log_file.close()\n",
    "    concat_lfp_dat(session,path_dat,subsessions)\n",
    "else:\n",
    "    with open(f'{session}-preprocess.log','a') as log_file:\n",
    "        now = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "        log_file.write(f'\\n{now} : Dat concatenation was skipped\\n')\n",
    "        log_file.close()\n",
    "print('Dat concatenation done in',time.time()-t,'s')\n",
    "\n",
    "with open(f'{session}-preprocess.log','a') as log_file:\n",
    "    log_file.write(f'Dat concatenation done in{time.time()-t} s\\n')\n",
    "    log_file.close()\n",
    "\n",
    "##################\n",
    "#DigitalIN CONCAT#\n",
    "##################    \n",
    "t = time.time()\n",
    "if writeDigital:\n",
    "    with open(f'{session}-preprocess.log','a') as log_file:\n",
    "        now = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "        log_file.write(f'\\n{now} : Starting digitalin concatenation\\n')\n",
    "        log_file.close()\n",
    "    concat_digitalin(session,path_digitalin,path_videos)\n",
    "else:\n",
    "    with open(f'{session}-preprocess.log','a') as log_file:\n",
    "        now = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "        log_file.write(f'\\n{now} : Digitalin concatenation was skipped\\n')\n",
    "        log_file.close()\n",
    "print('Digital in concatenation done in',time.time()-t,'s')\n",
    "\n",
    "with open(f'{session}-preprocess.log','a') as log_file:\n",
    "    log_file.write(f'Digitalin concatenation done in {time.time()-t} s\\n')\n",
    "    log_file.close()\n",
    "\n",
    "\n",
    "#############\n",
    "#LFP Extract#\n",
    "#############         \n",
    "t = time.time()\n",
    "if extractLFP: \n",
    "    with open(f'{session}-preprocess.log','a') as log_file:\n",
    "        now = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "        log_file.write(f'\\n{now} : Starting LFP Extraction\\n')\n",
    "        log_file.close()\n",
    "    downsampleDatFileBK(path,nChannels,fs)\n",
    "else:\n",
    "    with open(f'{session}-preprocess.log','a') as log_file:\n",
    "        now = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "        log_file.write(f'\\n{now} : LFP Extraction Skipped\\n')\n",
    "        log_file.close()\n",
    "\n",
    "print('LFP extraction done in',time.time()-t,'s')\n",
    "with open(f'{session}-preprocess.log','a') as log_file:\n",
    "    log_file.write(f'LFP extraction done in {time.time()-t} s\\n')\n",
    "    log_file.close()\n",
    "\n",
    "time.sleep(0.1)\n",
    "\n",
    "\n",
    "len_dat = os.path.getsize(session+'.dat')/(fs*nChannels*bytePerSample)\n",
    "len_digitalin = os.path.getsize('digitalin.dat')/(fs*bytePerSample)\n",
    "len_lfp = os.path.getsize(session+'.lfp')/(1250*nChannels*bytePerSample)\n",
    "\n",
    "print('Session\\'s length base on LFP dat file',len_dat,'s')\n",
    "print('Session\\'s length base on digitalin.dat file',len_digitalin,'s')\n",
    "print('Session\\'s length base on downsampled file',len_lfp,'s')\n",
    "\n",
    "\n",
    "with open(f'{session}-preprocess.log','a') as log_file:\n",
    "    now = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "    log_file.write(f'\\n\\n{now} : CHECKING INTEGRITY OF THE DATA AFTER COPY\\n')\n",
    "    log_file.write(f'Session s length base on LFP dat file {len_dat} s\\n')\n",
    "    log_file.write(f'Session s length base on digitalin.dat file {len_digitalin} s\\n')\n",
    "    log_file.write(f'Session s length base on downsampled file {len_lfp} s\\n')\n",
    "    log_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e36ece46",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Y:\\Rat25\\Rat25-210407'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20fc9146",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_lfp_dat(session,path_dat,subsessions):\n",
    "    \n",
    "    print('Starting dat concatenation')\n",
    "    originalFileSize = []\n",
    "    for p in path_dat:\n",
    "        originalFileSize.append(os.path.getsize(p))\n",
    "    finalSize = np.sum(originalFileSize)\n",
    "    concatSize = os.path.getsize(session+'.dat')\n",
    "    \n",
    "    \n",
    "    if finalSize == concatSize: \n",
    "        print('Concat Succefull')\n",
    "        print('Creating cat event file')\n",
    "        \n",
    "        t_rec = [0]\n",
    "        for p in path_dat:\n",
    "            t = os.path.getsize(p)/(fs*nChannels*bytePerSample)\n",
    "            print(t)\n",
    "            t_rec.append(t)\n",
    "        concat_event = np.cumsum(t_rec)*1000 # 1000 is for conversion in ms\n",
    "        \n",
    "        with open(session+'.cat.evt','w') as f:\n",
    "            print(concat_event)\n",
    "            print(subsessions)\n",
    "            for i,s in enumerate(subsessions):\n",
    "                print(i)\n",
    "                f.write(str(concat_event[i])+ f' {subsessions[i]} Start \\n')\n",
    "                f.write(str(concat_event[i+1])+ f' {subsessions[i]} End \\n')\n",
    "        return 1\n",
    "    else:\n",
    "        print('Error, please try again. Check freespace on your drives. You need at least',np.sum(originalFileSize)/1e9,'GB')\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ab5f2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dat concatenation\n",
      "Concat Succefull\n",
      "Creating cat event file\n",
      "12687.552\n",
      "14609.016\n",
      "[       0. 12687552. 27296568.]\n",
      "['pre_sleep_1', 'post_sleep_1']\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_lfp_dat(session,path_dat,subsessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3626d5c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
