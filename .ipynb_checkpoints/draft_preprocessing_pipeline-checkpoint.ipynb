{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import cv2\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_digitalin(path,nchannels=16,Fs = 20000):\n",
    "    import pandas as pd\n",
    "    \n",
    "    digital_word = np.fromfile(path,'uint16')\n",
    "    sample = len(digital_word)\n",
    "    time = np.arange(0,sample)\n",
    "    time = time/Fs\n",
    "\n",
    "    \n",
    "    for i in range(nchannels):\n",
    "        if i == 0: data = (digital_word & 2**i)>0\n",
    "        else: data = np.vstack((data,(digital_word & 2**i)>0))\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def CountTTL(TTL):\n",
    "    #Return the number TTL and the index where the first one starts\n",
    "    \n",
    "    TTL = list(map(int,TTL))\n",
    "    diff_TTL = np.diff(TTL)\n",
    "    \n",
    "    t_start = np.where(diff_TTL == 1)\n",
    "\n",
    "    return(len(t_start[0]),t_start[0][-1])\n",
    "\n",
    "\n",
    "def CountFrames(path):\n",
    "    #Return the number of frames inside a video\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    return(int(cap.get(cv2.CAP_PROP_FRAME_COUNT)))\n",
    "\n",
    "def TTLtoTimes(TTL,Fs = 20000):\n",
    "    \n",
    "    if isinstance(TTL[0],(np.bool_,bool)):\n",
    "        TTL = list(map(int,TTL))\n",
    "    \n",
    "    diff_TTL = np.diff(TTL)\n",
    "    print(diff_TTL.shape)\n",
    "    t_start = np.where(diff_TTL == 1)\n",
    "    t_end = np.where(diff_TTL == -1)\n",
    "    t_TTL = np.array([np.mean(interval,0) for interval in zip(t_start,t_end)])[0]\n",
    "    \n",
    "    return t_TTL/Fs\n",
    "\n",
    "def xml(session):\n",
    "    tree = ET.parse(session+'.xml')\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    xmlInfo = {}\n",
    "    for elem in root:\n",
    "        for subelem in elem:\n",
    "            try: \n",
    "                xmlInfo.update({subelem.tag:int(subelem.text)})\n",
    "            except:\n",
    "                pass\n",
    "    return xmlInfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main concat functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_lfp_dat(session,path_dat):\n",
    "    \n",
    "    print('Starting dat concatenation')\n",
    "    \n",
    "    to_cat = \" + \".join(path_dat)\n",
    "    p = subprocess.Popen('copy /B ' + to_cat + \" \" + session+'.dat&',shell = True)\n",
    "    \n",
    "    #Check if concatenated file is not corrupted : \n",
    "    originalFileSize = []\n",
    "    for p in path_dat:\n",
    "        originalFileSize.append(os.path.getsize(p))\n",
    "    finalSize = np.sum(originalFileSize)\n",
    "    f = False\n",
    "    while not f:\n",
    "        f = os.path.exists(session+'.dat')\n",
    "        \n",
    "    pbar = tqdm(total = finalSize,desc = 'Concat : ')\n",
    "    concatSize = 0\n",
    "    while concatSize < finalSize:\n",
    "        time.sleep(10)\n",
    "        s = os.path.getsize(session+'.dat')\n",
    "        \n",
    "        if concatSize == s:\n",
    "            print('Concatenation in stuck try again')\n",
    "            p.kill()\n",
    "            break\n",
    "        \n",
    "        pbar.update(s-concatSize)\n",
    "        concatSize = s\n",
    " \n",
    "    originalFileSize = []\n",
    "    for p in path_dat:\n",
    "        originalFileSize.append(os.path.getsize(p))\n",
    "    finalSize = np.sum(originalFileSize)\t\n",
    "    concatSize = os.path.getsize(session+'.dat')\n",
    "    \n",
    "    \n",
    "    if finalSize == concatSize: \n",
    "        print('Concat Succefull')\n",
    "        print('Creating cat event file')\n",
    "        \n",
    "        t_rec = []\n",
    "        for p in path_dat:\n",
    "            t = os.path.getsize(p)/(fs*nChannels*bytePerSample)\n",
    "            print(t)\n",
    "            t_rec.append(t)\n",
    "        concat_event = np.cumsum(t_rec)*1000 # 1000 is for conversion in ms\n",
    "        \n",
    "        with open(session+'.cat.evt','w') as f:\n",
    "            for c in concat_event:\n",
    "                f.write(str(c)+ ' cat\\n')\n",
    "        return 1\n",
    "    else:\n",
    "        print('Error, please try again. Check freespace on your drives. You need at least',np.sum(originalFileSize)/1e9,'GB')\n",
    "        return 0\n",
    "\n",
    "def concat_digitalin(session,path_digitalin,path_videos,nchannels = 16):\n",
    "    \n",
    "    byteSize = 2\n",
    "    print('Starting digitalin concatenation')\n",
    "    \n",
    "    originalFileSize = []\n",
    "    for p in path_digitalin:\n",
    "        originalFileSize.append(os.path.getsize(p))\n",
    "    finalSize = np.sum(originalFileSize)\n",
    "    \n",
    "    \n",
    "    data = np.empty((16,0),dtype = np.bool)\n",
    "    \n",
    "    pbar = tqdm(total = int(finalSize/byteSize))\n",
    "    # Reading all digitalin files in sub-session and comparate number of TTL with number of frames : \n",
    "    for p_digitalin,p_video in zip(path_digitalin,path_videos):\n",
    "        data_ = load_digitalin(p_digitalin,nchannels)\n",
    "        nTTL,lTTL = CountTTL(data_[0,:])\n",
    "        nFrames = CountFrames(p_video)\n",
    "\n",
    "        if nTTL == nFrames: # Same number of frames and TTL is easy case, we don't do anaything\n",
    "            print('NTTL',nTTL)\n",
    "            print('Number of Frames',nFrames)\n",
    "            print('Do not need to correct last TTL')\n",
    "        elif nTTL-1 == nFrames: # If NTTL and Frames are only one appart we need to delete the last TTL\n",
    "            print('NTTL',nTTL)\n",
    "            print('Number of Frames',nFrames)\n",
    "            print('Last TTL was at ',lTTL)\n",
    "            print('Correcting TTLs :')\n",
    "            data_[0,lTTL:] = False\n",
    "            print('Counting new number of TTL : ' , CountTTL(data_[0,:])[0])\n",
    "        else:\n",
    "            print('Some file might be corrupted, need to implement this')\n",
    "\n",
    "        # Anyway we concatenate digitalin in the RAM\n",
    "        print('Concatenating digitalin in RAM')\n",
    "        data = np.hstack((data,data_))\n",
    "        time.sleep(0.1)\n",
    "        pbar.update(np.size(data_))\n",
    "        time.sleep(0.1)\n",
    "\n",
    "\n",
    "    del data_ #delete temp var for better managing of RAM\n",
    "\n",
    "    #Recreate the write format of digital in (16channels inside a 16bits uint16)\n",
    "    digitalwrite = np.empty(data.shape[1],dtype = np.uint16)\n",
    "    for i in range(nchannels):\n",
    "        digitalwrite += np.uint16((data[i,:] * 2**i))\n",
    "\n",
    "    print('Writting digitalin.dat')\n",
    "    f = open('digitalin.dat','wb')\n",
    "    f.write(digitalwrite)\n",
    "    f.close()\n",
    "    \n",
    "    \n",
    "    #Check if concatenated file is not corrupted : \n",
    "\n",
    "    concatSize = os.path.getsize('digitalin.dat')\n",
    "    \n",
    "    if finalSize == concatSize: \n",
    "        print('Concat Succefull')\n",
    "        \n",
    "        \n",
    "        timestamps = TTLtoTimes(data[0,:])\n",
    "        np.save(session+'-frametime',timestamps)\n",
    "        del timestamps\n",
    "        del data\n",
    "        return 1   \n",
    "    else:\n",
    "        print('Error, please try again')\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsampleDatFileBK(path, n_channels, fs):\n",
    "    \n",
    "    #Code by Guillaume Viejo \n",
    "    \n",
    "    #Adapted by Billel Khouader oct 2020\n",
    "    \n",
    "    \"\"\"\n",
    "    downsample .dat file to .lfp 1/16 (20000 -> 1250 Hz)\n",
    "    \n",
    "    Since .dat file can be very big, the strategy is to load one channel at the time,\n",
    "    downsample it, and free the memory.\n",
    "    \n",
    "    BK : Modified in order to load dat chunk by chunk in write it as LFP file without \n",
    "    loading nChannels time the original dat file\n",
    "    \n",
    "    Args:\n",
    "        path: string\n",
    "        n_channel: int\n",
    "        fs: int\n",
    "    Return: \n",
    "        none\n",
    "    \"\"\"    \n",
    "    if not os.path.exists(path):\n",
    "        print(\"The path \"+path+\" doesn't exist; Exiting ...\")\n",
    "        sys.exit()\n",
    "    listdir     = os.listdir(path)\n",
    "    datfile     = [f for f in listdir if (f.startswith('Rat') & f.endswith('.dat'))]\n",
    "    if not len(datfile):\n",
    "        print(\"Folder contains no xml files; Exiting ...\")\n",
    "        sys.exit()\n",
    "    new_path = os.path.join(path, datfile[0])\n",
    "\n",
    "    f             = open(new_path, 'rb')\n",
    "    startoffile   = f.seek(0, 0)\n",
    "    endoffile     = f.seek(0, 2)\n",
    "    bytes_size    = 2\n",
    "    n_samples     = int((endoffile-startoffile)/n_channels/bytes_size)\n",
    "    duration      = n_samples/fs\n",
    "    f.close()\n",
    "\n",
    "    chunksize = 100_000\n",
    "    \n",
    "    lfp = np.zeros((int(n_samples/16),n_channels),dtype = np.int16)\n",
    "    \n",
    "    # Loading\n",
    "    \n",
    "    count = 0\n",
    "    f_lfp = open(session+'.lfp', 'wb')\n",
    "    \n",
    "    pbar = tqdm(total = n_samples,desc = 'Extracting LFP file from DAT file (sample)')\n",
    "    while count < n_samples:\n",
    "#         print(count)\n",
    "        f             = open(new_path, 'rb')\n",
    "        seekstart     = count*n_channels*bytes_size\n",
    "        f.seek(seekstart)\n",
    "        block         = np.fromfile(f, np.int16, n_channels*np.minimum(chunksize, n_samples-count))\n",
    "        f.close()\n",
    "        block         = block.reshape(np.minimum(chunksize, n_samples-count), n_channels)\n",
    "        count         += chunksize\n",
    "        \n",
    "    # Downsampling        \n",
    "#         lfp     = scipy.signal.resample_poly(block, 1, 16)\n",
    "        lfp     = scipy.signal.decimate(block,16,axis = 0)\n",
    "#         lfp     = scipy.signal.decimate(lfp,4,axis = 0)\n",
    "#         lfp     = scipy.signal.resample(block, int(len(block)/16))\n",
    "\n",
    "        f_lfp.write(lfp.astype('int16'))\n",
    "        pbar.update(chunksize)\n",
    "\n",
    "        \n",
    "        del block\n",
    "        del lfp\n",
    "    f_lfp.close()  \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking for files in sub-sessions : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5  dat files were found\n",
      "D:\\Rat03\\Rat03-20201007\\Rat03_201007_123536\\amplifier_analogin_auxiliary_int16.dat\n",
      "D:\\Rat03\\Rat03-20201007\\Rat03_201007_155352\\amplifier_analogin_auxiliary_int16.dat\n",
      "D:\\Rat03\\Rat03-20201007\\Rat03_201007_160125\\amplifier_analogin_auxiliary_int16.dat\n",
      "D:\\Rat03\\Rat03-20201007\\Rat03_201007_160725\\amplifier_analogin_auxiliary_int16.dat\n",
      "D:\\Rat03\\Rat03-20201007\\Rat03_201007_182654\\amplifier_analogin_auxiliary_int16.dat\n",
      "It will be concatenated in this order\n",
      "5  digitalin files were found\n",
      "D:\\Rat03\\Rat03-20201007\\Rat03_201007_123536\\digitalin.dat\n",
      "D:\\Rat03\\Rat03-20201007\\Rat03_201007_155352\\digitalin.dat\n",
      "D:\\Rat03\\Rat03-20201007\\Rat03_201007_160125\\digitalin.dat\n",
      "D:\\Rat03\\Rat03-20201007\\Rat03_201007_160725\\digitalin.dat\n",
      "D:\\Rat03\\Rat03-20201007\\Rat03_201007_182654\\digitalin.dat\n",
      "5  Videos were founds : \n",
      "D:\\Rat03\\Rat03-20201007\\Rat03_201007_123536\\Basler_acA1300-200uc__23039139__20201007_123538516.mp4\n",
      "D:\\Rat03\\Rat03-20201007\\Rat03_201007_155352\\Basler_acA1300-200uc__23039139__20201007_155353733.mp4\n",
      "D:\\Rat03\\Rat03-20201007\\Rat03_201007_160125\\Basler_acA1300-200uc__23039139__20201007_160127147.mp4\n",
      "D:\\Rat03\\Rat03-20201007\\Rat03_201007_160725\\Basler_acA1300-200uc__23039139__20201007_160727369.mp4\n",
      "D:\\Rat03\\Rat03-20201007\\Rat03_201007_182654\\Basler_acA1300-200uc__23039139__20201007_182658367.mp4\n"
     ]
    }
   ],
   "source": [
    "path = 'D:\\\\Rat03\\\\Rat03-20201007'\n",
    "session = path.rsplit('\\\\')[-1]\n",
    "os.chdir(path)\n",
    "\n",
    "# Looking for Dat (LFPs) files \n",
    "path_dat = [[os.path.join(path,p,f) \n",
    "             for f in os.listdir(os.path.join(path,p)) if f.startswith(\"amplifier_analogin_auxiliary\") & f.endswith('.dat')][0] \n",
    "            for p in os.listdir(path) if os.path.isdir(p)]\n",
    "print(len(path_dat),' dat files were found')\n",
    "for p in path_dat: print(p)\n",
    "print('It will be concatenated in this order')\n",
    "\n",
    "\n",
    "# Looking for digitalin.dat (TTLs and all digital signal)\n",
    "path_digitalin = [[os.path.join(path,p,f) \n",
    "             for f in os.listdir(os.path.join(path,p)) if f.startswith(\"digitalin\") & f.endswith('.dat')][0] \n",
    "            for p in os.listdir(path) if os.path.isdir(p)]\n",
    "print(len(path_digitalin),' digitalin files were found')\n",
    "for p in path_digitalin: print(p)\n",
    "    \n",
    "\n",
    "#Looking for videos (usefull for counting frames)\n",
    "path_videos = [[os.path.join(path,p,f) \n",
    "             for f in os.listdir(os.path.join(path,p)) if f.startswith(\"Basler\") & f.endswith('.mp4')][0] \n",
    "            for p in os.listdir(path) if os.path.isdir(p)]\n",
    "print(len(path_videos),\" Videos were founds : \")\n",
    "for p in path_videos: print(p)\n",
    "    \n",
    "    \n",
    "\n",
    "if not (len(path_dat)==len(path_digitalin)==len(path_videos)): \n",
    "    print('Not same number of dat, digitalin or videos, please check integrity of each subsession')\n",
    "    raise ValueError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenating LFPs Dat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Concat : 100%|█████████████████████████████████████████████████| 95486834880/95486834880 [41:26<00:00, 23631539.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concat Succefull\n"
     ]
    }
   ],
   "source": [
    "xmlInfo = xml(session)\n",
    "\n",
    "fs = xmlInfo['samplingRate']\n",
    "nChannels = xmlInfo['nChannels']\n",
    "bytePerSample = xmlInfo['nBits']/8\n",
    "\n",
    "\n",
    "files = os.listdir()\n",
    "if session+'.dat' in files:\n",
    "    overwrite = input(session+'.dat' + ' already exist do you want to overwrite ? (Y/N)')\n",
    "    if overwrite.lower() == \"y\": writeDat = True\n",
    "    else: \n",
    "        writeDat = False\n",
    "        print('Concatenation of ',session+'.dat aborted')\n",
    "else: writeDat = True\n",
    "\n",
    "if writeDat: concat_lfp_dat(session,path_dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenating digitalin files.\n",
    "\n",
    "A little bit complicated. We need to remove last TTL sometimes because the last frame is not always recorded : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "digitalin.dat already exist do you want to overwrite ? (Y/N) y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concat Succefull\n"
     ]
    }
   ],
   "source": [
    "nchannels = 16 #Should always be 16 if using itan software ? \n",
    "\n",
    "files = os.listdir()\n",
    "if 'digitalin.dat' in files:\n",
    "    overwrite = input('digitalin.dat' + ' already exist do you want to overwrite ? (Y/N)')\n",
    "    if overwrite.lower() == \"y\": writeDigital = True\n",
    "    else: \n",
    "        writeDigital = False\n",
    "        print('Concatenation of digitalin aborted')\n",
    "else: writeDigital = True\n",
    "\n",
    "if writeDigital: concat_digitalin(session,path_digitalin,path_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session's length base on LFP dat file 23403.636 s\n",
      "Session's length base on digitalin.dat file 23403.636 s\n"
     ]
    }
   ],
   "source": [
    "print('Session\\'s length base on LFP dat file',os.path.getsize('Rat03-20201007.dat')/(fs*102*bytePerSample),'s')\n",
    "print('Session\\'s length base on digitalin.dat file',os.path.getsize('digitalin.dat')/(fs*bytePerSample),'s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exctract LFP files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5  dat files were found\n",
      "D:\\Rat03\\Rat03-20201007\\Rat03_201007_123536\\amplifier_analogin_auxiliary_int16.dat\n",
      "D:\\Rat03\\Rat03-20201007\\Rat03_201007_155352\\amplifier_analogin_auxiliary_int16.dat\n",
      "D:\\Rat03\\Rat03-20201007\\Rat03_201007_160125\\amplifier_analogin_auxiliary_int16.dat\n",
      "D:\\Rat03\\Rat03-20201007\\Rat03_201007_160725\\amplifier_analogin_auxiliary_int16.dat\n",
      "D:\\Rat03\\Rat03-20201007\\Rat03_201007_182654\\amplifier_analogin_auxiliary_int16.dat\n",
      "It will be concatenated in this order\n",
      "5  digitalin files were found\n",
      "D:\\Rat03\\Rat03-20201007\\Rat03_201007_123536\\digitalin.dat\n",
      "D:\\Rat03\\Rat03-20201007\\Rat03_201007_155352\\digitalin.dat\n",
      "D:\\Rat03\\Rat03-20201007\\Rat03_201007_160125\\digitalin.dat\n",
      "D:\\Rat03\\Rat03-20201007\\Rat03_201007_160725\\digitalin.dat\n",
      "D:\\Rat03\\Rat03-20201007\\Rat03_201007_182654\\digitalin.dat\n",
      "5  Videos were founds : \n",
      "D:\\Rat03\\Rat03-20201007\\Rat03_201007_123536\\Basler_acA1300-200uc__23039139__20201007_123538516.mp4\n",
      "D:\\Rat03\\Rat03-20201007\\Rat03_201007_155352\\Basler_acA1300-200uc__23039139__20201007_155353733.mp4\n",
      "D:\\Rat03\\Rat03-20201007\\Rat03_201007_160125\\Basler_acA1300-200uc__23039139__20201007_160127147.mp4\n",
      "D:\\Rat03\\Rat03-20201007\\Rat03_201007_160725\\Basler_acA1300-200uc__23039139__20201007_160727369.mp4\n",
      "D:\\Rat03\\Rat03-20201007\\Rat03_201007_182654\\Basler_acA1300-200uc__23039139__20201007_182658367.mp4\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(path):\n",
    "    print(path, ' does not exists please check directory','Exiting...')\n",
    "    sys.exit()\n",
    "\n",
    "session = path.rsplit('\\\\')[-1]\n",
    "os.chdir(path)\n",
    "\n",
    "# Looking for Dat (LFPs) files \n",
    "path_dat = [[os.path.join(path,p,f) \n",
    "             for f in os.listdir(os.path.join(path,p)) if f.startswith(\"amplifier_analogin_auxiliary\") & f.endswith('.dat')][0] \n",
    "            for p in os.listdir(path) if os.path.isdir(p)]\n",
    "print(len(path_dat),' dat files were found')\n",
    "for p in path_dat: print(\"    \",p)\n",
    "print('It will be concatenated in this order')\n",
    "\n",
    "\n",
    "# Looking for digitalin.dat (TTLs and all digital signal)\n",
    "path_digitalin = [[os.path.join(path,p,f) \n",
    "             for f in os.listdir(os.path.join(path,p)) if f.startswith(\"digitalin\") & f.endswith('.dat')][0] \n",
    "            for p in os.listdir(path) if os.path.isdir(p)]\n",
    "print(len(path_digitalin),' digitalin files were found')\n",
    "for p in path_digitalin: print(\"    \",p)\n",
    "\n",
    "\n",
    "#Looking for videos (usefull for counting frames)\n",
    "path_videos = [[os.path.join(path,p,f) \n",
    "             for f in os.listdir(os.path.join(path,p)) if f.startswith(\"Basler\") & f.endswith('.mp4')][0] \n",
    "            for p in os.listdir(path) if os.path.isdir(p)]\n",
    "print(len(path_videos),\" Videos were founds : \")\n",
    "for p in path_videos: print(\"    \",p)\n",
    "\n",
    "if not (len(path_dat)==len(path_digitalin)==len(path_videos)): \n",
    "    print('Not same number of dat, digitalin or videos, please check integrity of each subsession')\n",
    "    raise ValueError\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "xmlInfo = xml(session)\n",
    "\n",
    "fs = xmlInfo['samplingRate']\n",
    "nChannels = xmlInfo['nChannels']\n",
    "bytePerSample = xmlInfo['nBits']/8\n",
    "\n",
    "\n",
    "files = os.listdir()\n",
    "if session+'.dat' in files:\n",
    "    overwrite = input(session+'.dat' + ' already exist do you want to overwrite ? (Y/N)')\n",
    "    if overwrite.lower() == \"y\": writeDat = True\n",
    "    else: \n",
    "        writeDat = False\n",
    "        print('Concatenation of ',session+'.dat aborted')\n",
    "else: writeDat = True\n",
    "\n",
    "if 'digitalin.dat' in files:\n",
    "    overwrite = input('digitalin.dat' + ' already exist do you want to overwrite ? (Y/N)')\n",
    "    if overwrite.lower() == \"y\": writeDigital = True\n",
    "    else: \n",
    "        writeDigital = False\n",
    "        print('Concatenation of digitalin aborted')\n",
    "else: writeDigital = True\n",
    "\n",
    "\n",
    "if session+'.lfp' in files:\n",
    "    overwrite = input(session+'.lfp' + ' already exist do you want to overwrite ? (Y/N)')\n",
    "    if overwrite.lower() == \"y\": extractLFP = True\n",
    "    else: \n",
    "        extractLFP = False\n",
    "        print('Concatenation of digitalin aborted')\n",
    "else: extractLFP = True\n",
    "\n",
    "t = time.time()\n",
    "if writeDat: concat_lfp_dat(session,path_dat)\n",
    "print('Dat concatenation done in',time.time()-t,'s')\n",
    "\n",
    "t = time.time()\n",
    "if writeDigital: concat_digitalin(session,path_digitalin,path_videos)\n",
    "print('Digital in concatenation done in',time.time()-t,'s')\n",
    "\n",
    "t = time.time()\n",
    "if extractLFP: downsampleDatFileBK(path,nChannels,fs)\n",
    "print('LFP extraction done in',time.time()-t,'s')\n",
    "\n",
    "\n",
    "print('Session\\'s length base on LFP dat file',os.path.getsize(session+'.dat')/(fs*nChannels*bytePerSample),'s')\n",
    "print('Session\\'s length base on digitalin.dat file',os.path.getsize('digitalin.dat')/(fs*bytePerSample),'s')\n",
    "print('Session\\'s length base on downsampled file',os.path.getsize(session+'.lfp')/(1250*nChannels*bytePerSample),'s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
